* Error, Accuracy, and Stability
  - integers are exact
  -- operations involving them are exact, given range
 ** Floats
    Float represented internally as follows:

    @math
    S ⨯  M ⨯  b^(E-e) ⨯  1.F
    @end
    - *S* is the sign
    - *M* /is the mantissa, which represents the digits to the right of the decimal point/
    - *b* /is the base, in general base 2 for binary/
    - *E* /is the exponent to raise to, for floats up to 255, (255 representing infinity)/
    - *e* /is the/ bias /of the exponent, a fixed integer/
    -- More on Mantissa:
    --- *M* /= 1.F/ where /F/ is the fraction  
  

    - This solution is not unique for all numbers
    -- To guarantee uniqueness, we define a *normalized* float:
    -- *normalized* /means the float is shifted to the left by as far as possible, meaning the bit on the left is always 1/ 

  @code
  0 10000000001 1010 (+ 48 more zeros) = +1 ⨯ 2^(1025-1023) x1.1010_2 = 6.5
  @end
  To get the floating point rep in C brand code:
  @code c

  #include <cstdio>
  union Udoub {
    double d;
    unsigned char c[8];
  };
  int main() {
    Udoub u;
    u.d = 6.5;
    for (int i=7;i>=0;i--) printf("%02x",u.c[i]);
    printf("\n");
    return 0;
  }
}
  @end
  
  - After you pass the smalllest normalizable value (very negative exponent), you start getting into the unnormalized zone and right shifting the mantissa, which lets you gradually underflow to zero 23-52 bits later, instead of abruptly
 ** Roundoff error
    - Operations involving floating point numbers is inexact
    $ Addition Example
    Let there be 2 floating numbers, A and B. Let A.Mantissa > B.Mantissa. B.Mantissa is right shifted (divided by 2), while B.exponent is raised until B.Exponent = A.exponent. This means that some of the lower order (least significant) bits of B are lost. If the numbers are too far apart in magnitude, the smaller operand is right shifted into oblivion
    - *machine accuracy ϵ_m* /is th smallest in magnitude floating point number which when added to 1.0 is different from 1.0/
   -- in most computers it is about 2.22 x 10^-16
   -- *ϵ_m* /roughly represents the fractional accuracy to which floating point numbers are represented, corresponding to a change of one in the least significant bit/
   - *Any  arithmetic operation involving floats induces a fractional error of at least ϵ_m*

