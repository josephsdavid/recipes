* Error, Accuracy, and Stability
  - integers are exact
  -- operations involving them are exact, given range
 ** Floats
    Float represented internally as follows:

    @math
    S ⨯  M ⨯  b^(E-e) ⨯  1.F
    @end
    - *S* is the sign
    - *M* /is the mantissa, which represents the digits to the right of the decimal point/
    - *b* /is the base, in general base 2 for binary/
    - *E* /is the exponent to raise to, for floats up to 255, (255 representing infinity)/
    - *e* /is the/ bias /of the exponent, a fixed integer/
    -- More on Mantissa:
    --- *M* /= 1.F/ where /F/ is the fraction  
  

    - This solution is not unique for all numbers
    -- To guarantee uniqueness, we define a *normalized* float:
    -- *normalized* /means the float is shifted to the left by as far as possible, meaning the bit on the left is always 1/ 

  @code
  0 10000000001 1010 (+ 48 more zeros) = +1 ⨯ 2^(1025-1023) x1.1010_2 = 6.5
  @end
  To get the floating point rep in C brand code:
  @code c

  #include <cstdio>
  union Udoub {
    double d;
    unsigned char c[8];
  };
  int main() {
    Udoub u;
    u.d = 6.5;
    for (int i=7;i>=0;i--) printf("%02x",u.c[i]);
    printf("\n");
    return 0;
  }
}
  @end
  
  - After you pass the smalllest normalizable value (very negative exponent), you start getting into the unnormalized zone and right shifting the mantissa, which lets you gradually underflow to zero 23-52 bits later, instead of abruptly
 ** Roundoff error
    - Operations involving floating point numbers is inexact
    $ Addition Example
    Let there be 2 floating numbers, A and B. Let A.Mantissa > B.Mantissa. B.Mantissa is right shifted (divided by 2), while B.exponent is raised until B.Exponent = A.exponent. This means that some of the lower order (least significant) bits of B are lost. If the numbers are too far apart in magnitude, the smaller operand is right shifted into oblivion

    - *machine accuracy ϵ_m* /is th smallest in magnitude floating point number which when added to 1.0 is different from 1.0/. 
     -- in most computers it is about 2.22 x 10^-16 
    -- *ϵ_m* /roughly represents the fractional accuracy to which floating point numbers are represented, corresponding to a change of one in the least significant bits
    *Any  arithmetic operation involving floats induces a fractional error of at least ϵ_m*. 
    - This is known as roundoff error
    ^ Note on Roundoff error
    ϵ_m is not the smallest representable number! ϵ_m depends on number of bits in mantissa, smallest possible depends on number of bits in the exponent!
  *** Python example

      @code python

      import numpy as np


      def calculate_eps():
          eps = 1
          n = 1

          while n != n+eps:
              eps *= 1e-1
          eps*= 10
          print(eps)

      def calculate_eps_np():
          eps = np.float128(1)
          n = np.float128(1)

          while n != n+eps:
              eps *= np.float128(1e-1)
          eps*= np.float128(10)
          print(eps)

      if __name__ == "__main__":
          calculate_eps()
          calculate_eps_np()
      else:
          pass

      @end
      ---

    Roundoffs accumulate the more  calculations you do!
    If you perform /N/ calculations, best case scenario (if roundoff is randomly up or down), you will have a total roundoff error of /√N ϵ_m/ (sqrt comes from a random walk). However, computers *and* calculations are typically going to prefer one direction over the other, which will raise the error size to the order of /Nϵ_m/. Sometimes you can really blow up the roundoff error by for example subtracting two nearly equal numbers resulting in a really small number. This is surprisingly common, for example the quadratic equation in conditions where b^2 >> ac
 ** Truncation error
    This is error induced by programmers to make algorithms go. It is discrepancy between real solution and what is calculated in practice. This is the sort of error that we want to avoid. Truncation errors would happen on an infinite precision computer!

 ** Stability
    Sometimes, the way that seems simple and good to do can be /unstable/, meaning that if any roundoff error gets mixed in to the result, it can explode! An example of this is calculating integer powers of ϕ the golden mean, very quickly explodes to being incorrect, even if ϕ^n is only like 10^-4

* C family syntax

  most things are normal, but `++`, `?`, `:`, assignment, and logical stuff is right to left

 ** C control structures
    - *For loop*
    @code c

    for (j=2;j<=1000;j++) {
      b[j]=a[j-1];
      a[j-1]=j;
    }
    @end
    - Conditional
    @code c 

    if (...) {
      ...
    } else if (...) {
      ...
    } else {
      ...
    }
           
    @end
    Note the braces are not necessary, but they make the code more easy to evaluate by eyesight. Else corresponds to the most recent if statement if not surrounded by braces!!
    - While
    @code c 

    while (n < 1000) {
      n *= 2;
      j += 1;
    }
     
    @end
    addin a do  in there means that even if conds of while loop are not initially satisfied, the code will run at least once:
    @code c 
    do {
      n += 2;
      j += 1;
    } while (n < 1000)
    @end
    - break
    @code c 

    for(;;) {
        ...
        if (...) break;
        ...
    }
    ...
         
    @end

 ** A note on one liners
    Dont be too tricky, dont be stupidly literal. It will make your life too hard to debug, or make it too hard to see the forest through the trees. Always write code that is /slightly less tricky/ than what you are willing to read!! Be idiomatic but not tricky!
 ** Structs and classes
    A struct basically is a class with all public vars and methods.
    Uses of objects 
    - group functions
    - make standard interface
    - Return multiple values by saving all useful results and let user grab what is nice
    - Save internal state to save calculation

    Use scoping to save memory, once scope is gone everything vanishes. Everything delimited by braces is a scope

 ** Functions and Functors
    Functors use magic overloading of operator() method?? Just like a function with globals aded to it?

    @code cpp
    struct Contimespow {
      Doub c, p;
      Contimespow(const Doub cc, const Doub pp) : c(cc), p(pp) {}
      Doub operator()(const Doub x) { return c * pow(x, p); }
    };
    @end
    And then you do:
    `Contimespow h(4.,0.5)` and pass that on to your routine
    - We can write routines to accept either functors or functions. A routine that accepts either function or functors uses a `template`
    @code cpp 

    template <class T> 
    Doub someQuadrature(T &func, const Doub a, const Doub b);
     
    @end
    Some conventions:
    - *Ftor* /functor/: `Ftor ftor(...);`
    - *Fbare* /plain function/: `Fbare fbare(...)`;

    More complicated example:
    @code cpp 
    template <class T> 
    struct SomeStruct { SomeStruct(T &func, ...); };
    @end
    We would instantion it with a functor like so:
    @code cpp 
    Ftor ftor;
    SomeStruct<Ftor> s(ftor, ...);
    SomeStruct<Doub (const Doub)> s(fbare, ...);
    @end

 ** Inheritance
    @code cpp 
    struct Binomialdev : Ran {}; 
    @end
    Prerequestit

* Vector and matrix objects
 








* References
  - {https://pythonnumericalmethods.berkeley.edu/notebooks/chapter09.03-Roundoff-Errors.html}[roundoff errors in python]
  - {https://betterexplained.com/articles/understanding-quakes-fast-inverse-square-root/}[magical quake stuff]
  




   

