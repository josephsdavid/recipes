* Error, Accuracy, and Stability
  - integers are exact
  -- operations involving them are exact, given range
 ** Floats
    Float represented internally as follows:

    @math
    S ⨯  M ⨯  b^(E-e) ⨯  1.F
    @end
    - *S* is the sign
    - *M* /is the mantissa, which represents the digits to the right of the decimal point/
    - *b* /is the base, in general base 2 for binary/
    - *E* /is the exponent to raise to, for floats up to 255, (255 representing infinity)/
    - *e* /is the/ bias /of the exponent, a fixed integer/
    -- More on Mantissa:
    --- *M* /= 1.F/ where /F/ is the fraction  
  

    - This solution is not unique for all numbers
    -- To guarantee uniqueness, we define a *normalized* float:
    -- *normalized* /means the float is shifted to the left by as far as possible, meaning the bit on the left is always 1/ 

  @code
  0 10000000001 1010 (+ 48 more zeros) = +1 ⨯ 2^(1025-1023) x1.1010_2 = 6.5
  @end
  To get the floating point rep in C brand code:
  @code c

  #include <cstdio>
  union Udoub {
    double d;
    unsigned char c[8];
  };
  int main() {
    Udoub u;
    u.d = 6.5;
    for (int i=7;i>=0;i--) printf("%02x",u.c[i]);
    printf("\n");
    return 0;
  }
}
  @end
  
  - After you pass the smalllest normalizable value (very negative exponent), you start getting into the unnormalized zone and right shifting the mantissa, which lets you gradually underflow to zero 23-52 bits later, instead of abruptly
 ** Roundoff error
    - Operations involving floating point numbers is inexact
    $ Addition Example
    Let there be 2 floating numbers, A and B. Let A.Mantissa > B.Mantissa. B.Mantissa is right shifted (divided by 2), while B.exponent is raised until B.Exponent = A.exponent. This means that some of the lower order (least significant) bits of B are lost. If the numbers are too far apart in magnitude, the smaller operand is right shifted into oblivion

    - *machine accuracy ϵ_m* /is th smallest in magnitude floating point number which when added to 1.0 is different from 1.0/. 
     -- in most computers it is about 2.22 x 10^-16 
    -- *ϵ_m* /roughly represents the fractional accuracy to which floating point numbers are represented, corresponding to a change of one in the least significant bits
    *Any  arithmetic operation involving floats induces a fractional error of at least ϵ_m*. 
    - This is known as roundoff error
    ^ Note on Roundoff error
    ϵ_m is not the smallest representable number! ϵ_m depends on number of bits in mantissa, smallest possible depends on number of bits in the exponent!
  *** Python example

      @code python

      import numpy as np


      def calculate_eps():
          eps = 1
          n = 1

          while n != n+eps:
              eps *= 1e-1
          eps*= 10
          print(eps)

      def calculate_eps_np():
          eps = np.float128(1)
          n = np.float128(1)

          while n != n+eps:
              eps *= np.float128(1e-1)
          eps*= np.float128(10)
          print(eps)

      if __name__ == "__main__":
          calculate_eps()
          calculate_eps_np()
      else:
          pass

      @end
      ---

    Roundoffs accumulate the more  calculations you do!
    If you perform /N/ calculations, best case scenario (if roundoff is randomly up or down), you will have a total roundoff error of /√N ϵ_m/ (sqrt comes from a random walk). However, computers *and* calculations are typically going to prefer one direction over the other, which will raise the error size to the order of /Nϵ_m/. Sometimes you can really blow up the roundoff error by for example subtracting two nearly equal numbers resulting in a really small number. This is surprisingly common, for example the quadratic equation in conditions where b^2 >> ac
 ** Truncation error







* References
  - {https://pythonnumericalmethods.berkeley.edu/notebooks/chapter09.03-Roundoff-Errors.html}[roundoff errors in python]
  - {https://betterexplained.com/articles/understanding-quakes-fast-inverse-square-root/}[magical quake stuff]
  




   

