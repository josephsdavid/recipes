@document.meta
  title: Solution of Linear Algebraic Equations
  description: 
  authors: david
  categories: 
  created: 2022-02-14
  version: 0.0.11
@end

=TOC
    
* Introduction
  Matrix representation of linear algebraic equations:

  @code 
                                                      ┌─      ──┐
      ┌─                                   ──┐        │  b0     │
      │ a0        a01        ...   a(0)(N-1) │        │  b1     │
   A =│ a10       a11        ...   a(0)(N-1) │    b = │  ...    │
      │ a(M-1)(0) a(M-1)(1)  ...   a(0)(N-1) │        │  ...    │
      └─                                   ──┘        │  b(M-1) │
                                                      └─      ──┘
  @end
  Then we can represent the equation as 
  @math  
  A ⬝ x  = b
  @end
  Dot can be represented as:
  @math  
  C = A ⬝ b ⟺   c_ik = ∑aijbjk
  @end
  In situations where M = N, we have only an ok chance of solving! Why?

 ** Nonsingular vs singular sets of equations
    > Why is M = N so difficult?
    - *row degeneracy* /when one or more of the equations is a linear combination of the others/
    - *column degeneracy* /some variables only exist in just one linear combination/
    The types of degeneracy imply each other. 
    - *singular* /set of equations that are degenerate/
    - if N is very large, answers will be wrong due to accumulated roundoff errors.
    -- The closer to singularity we are, the more likely this will happen, due to increasingly close cancellations. 
    - Sometimes, the equations may be so close to linear dependence that roundoff errors render them linearly dependent. This can be viewed as a special case of the previous issue.
    
    Most of the sophisitication of linear solvers is devoted to detecting and correcting these two issues.
 ** Tasks of Comp LA (topics in this chapter)
    With M = N:
    - solution of equation $A ⬝ x = b$ for x
    - solution of many matrix equations with A held constant for a set of vectors
    - Calculation of the inverse of A
    - calculation of determinant
    With M < N or M = N with degeneracy, there are fewer equations than unknowns. Either no solution, or more than one solution. In the latter event the solution space consists of a particular solution added to any linear combination. Available tasks:
    - SVD, which finds the solution space
    With M > N, there are more equations than unknowns, and the system is *overdetermined*. The best compromise solution here is to find the solution that comes closest to solving all equations simultaneously. If we define closeness as least squares, we arrive at the:
    - linear least sequares problem
    Other tasks:
    - iterative improvement of solutions
    - special forms of the linear algebra problems
    - fast matrix inversion

* Gauss-Jordan Elimination 
  -Otherwise known as Gaussian elimination.-

  $ Basic idea
  add or subtract linear combinations of the given equations until each equation contains only one of the unknowns, thus giving an immediate solution. Also sometimes used for calculating the inverse.

  For solving linear sets of equations, gaussian jordan elimination gets both the inverse A^-1 and the solution for the right hand side vectors. Has some problems though:
  ~ All RHS vectors b must be stored and manipulated simultaneously
  ~ When the inverse matrix is not desired, GJ elimination is 3 times slower than anything else.
  Strength is that it is very stable 

  For inverting a matrix, GJ elimination is about as efficient as any other direct method {^ Direct method:} {^ Indirect Method}. There is no reason not to use it if you want the matrix inverse. 

  Regarding  issue number 1 above, repeatedly using the inverse in a non simultaneous method could lead to an accumulation of roundoff errors.
  Therfore, GJ elimination should not be your top choice! Why are we discussing it then?
  > Because it is straightforward, solid, and discusses /pivoting/, which is important. 

 ** Elimination on Column-Augmented matrices

    | 2.1.1
    @code whiteboard

                                ┌─────                                      ─────┐
       ┌──                ──┐   │                          ┌──                ──┐│
       │ a00  a01  a02  a03 │   │  x00     x01     x02     │ y00  y01  y02  y03 ││
       │ a10  a11  a12  a13 │   │  x10     x11     x12     │ y10  y11  y12  y13 ││
       │ a20  a21  a22  a23 │ ⬝ │  x20  ⊔  x21  ⊔  x22  ⊔  │ y20  y21  y22  y23 ││
       │ a30  a31  a32  a33 │   │  x30     x31     x32     │ y30  y31  y32  y33 ││
       └──                ──┘   │                          └──                ──┘│
                                └────                                        ────┘

                                 ┌─────                              ─────┐ 
                                 │                          ┌──        ──┐│ 
                                 │  b00     b01     b02     │ 1  0  0  0 ││ 
                                 │  b10     b11     b12     │ 0  1  0  0 ││ 
                          =      │  b20  ⊔  b21  ⊔  b22  ⊔  │ 0  0  1  0 ││ 
                                 │  b30     b31     b32     │ 0  0  0  1 ││ 
                                 │                          └──        ──┘│ 
                                 └────                                ────┘ 
    @end                                                                                     
    The `(⬝)` dot represents matrix mult, while the operator ⊔ represents column augmentation. That  is making a wider matrix out of the little matrices. We can write this  as:
    @math  
    [A] ⬝ [x_0 ⊔ x_1 ⊔ x_2 ⊔ Y] = [b_0 ⊔ b_1 ⊔ b_2 ⊔ 1]
    @end
    Where *A* and *Y* are square matrices, the *b_i* and *x_i* are column vectors, and *1* is the identity. It should be pretty easy to see that this simultaneously solves:

    | 2.1.3
    @math  

    A ⬝x_0 = b_0    A⬝x_1 = b_1    A⬝x_2 = b_2 

                          
                  A⬝Y = 1 
                          

    @end

    - It is also pretty plain that changing rows of A and the corresponding rows of B does not mess up X and Y, just reorders the equations. 
    - Same  deal if we replace any row in a by a linear combination of itself and any other row, as long as B and 1 have the same linear combination operations. 
    - Interchanging any two columns of A gives the same solution if the right rows x and Y.
    GJ elimination uses these operations to reduce A to the identity matrix, therefore finding the right result

 ** Pivoting

  *** Gauss-Jordan Elimination without pivoting
      ~ zeroth row devided by $a_00$, which is a linear combination of row zero and 0 times any other row (A trivial combination). 
      ~ The right amount of the zeroth row is subtracted from all the other rows to make all the other a_io's zero. 
      ~~ The zeroth column of A now agrees with the identity
      ~ We move to column 1 and divide row 1 by a_11, then subtract the right amount of row 1 from rows 0, 2, and 3, to make their entries in column 1 0
      ~ And so on
      This method will run into trouble if it encounters a zero on the current diaganol when we do that, and it is numerically unstable in the presence of roundoff error!!! You must never do without pivoting!

  *** What on earth is pivoting??
      Just swapping rows (partial pivoting) or rows and columns(full pivoting), to make things line up right on the diagonal. Since we dont want to mess up the part of the identiy matrix we have already built, we can pivot among elements that are both 
      ~ on rows below or one the row we want to normalize
      ~ on columns to the right or on the column we want to eliminate
      Partial pivot easier than full pivot, dont need to keep track of changes to the solution vector. It only makes elements alreadyy in the correct column available, while full pivoting does more, but still is a little tricky.
      
      > How to recognize a desirable pivot?
      - Not fully known the very best way.
      - Pretty good is to pick the biggest available number 
      - Depends on original scaling of the equations
      -- If we take the last equation and multiply it by 1 million, it will likely contribute to the first pivot, however the underlying solution is not affected by this! Therefore sometimes we choose the pivot which /would/ have been the largest if the original equations had been scaled. This is called /implicit pivoting/
      The c++ code is {@ gaussj.h}[here].


* Gaussian Elimination with backsubstitiution
  This is primarily pedagogy. Somewhere between full elimination such as GJ and trianular decomposition. Gaussian elimination reducs a matrix halfway to the identity, a matrix whose components on the diagonal and above remain nontrivial.
  - say we are doing GJ elimiination, and at each stage subtract away rows only Before the then current pivot element. We then get an upper triangular matrix, having done gaussian elimination. Now we can do some magic
 ** backsubstitiution
    @code whiteboard
       ┌─                       ─┐   ┌─   ─┐     ┌─   ─┐
       │a'00   a'01   a'02   a'03│   │  x0 │     │ b'0 │
       │  0    a'11   a'12   a'13│ · │  x1 │  =  │ b'1 │
       │  0      0    a'22   a'23│   │  x2 │     │ b'2 │
       │  0      0      0    a'33│   │  x3 │     │ b'3 │
       └─                       ─┘   └─   ─┘     └─   ─┘
    @end
    The primes signify that the original numerical values are not there anymore! They have been modified and divided and subtracted! We can now simply solve for the x's:
    | Backsubstitiution
    @math  
    x3 = b'3/a'33 
    ∴ x2 = (1/a'22) * (b'2 - x3*a'23)
    ∴ xi = (1/a'ii) * (b'i - ∑_(j=i+1)^*(N-1) a'ij*xj)
    @end
    Just start with bottom right and move up.
    The advantage of this is that this does fewer operations/loops. please refer to the text for an in depth discussion of this.

 ** Disadvantages
    Both Gaussian methods share the same issue that we must know all the answers on the RHS.

* LU Decomposition

  Suppose we were able to write the matrix *A* as a product of two matrices:
  | Decomposition
  @math  
  L · U = A 
  @end
  Where:

  - *L* /Lower triangular (only elements on and below diagonal)/
  - *U* /Upper triangular (only elements on and above diagonal)/

  @code whiteboard

    ┌──                ──┐   ┌──                ──┐   ┌──                ──┐  
    │ α00   0    0    0  │   │ β00  β10  β02  β03 │   │ a00  a01  a02  a03 │  
    │ α10  α10   0    0  │ · │  0   β11  β12  β13 │ = │ a10  a11  a12  a13 │      
    │ α20  α20  α20   0  │   │  0    0   β22  β23 │   │ a20  a21  a22  a23 │  
    │ α30  α30  α30  α30 │   │  0    0    0   β33 │   │ a30  a31  a32  a33 │  
    └──                ──┘   └──                ──┘   └──                ──┘  


  @end

  We can use a decomposition like {| Decomposition}[this one] to solve the linear set:

  @math  
  A · x = b 
  A · x = (L · U) · x = b
  A · x = (L · U) · x  = L · (U · X) = b
  @end
  by solving for the vector y such that:

  | 2.3.4
  @math  
  L · y = b 
  @end
  and then solving
  | 2.3.5
  @math  
  U · x = y 
  @end

  If we break them into two little sub linear sets really helps us out. We saw in {| Backsubstitiution}[that we can easily solve an upper triangular system]. 
  We can first solve {| 2.3.4} as:
  @math  
              y0 = b0 / α00 
  
                 i-1
        1  ┌─     ⎲        ─┐
  yi =  ―  │ bi - ⎳ αij · yj│ i = 1,2,...,N-1
       αii │     j=0        │
           └─              ─┘


  @end
  Then we can solve {| 2.3.5} as:
  @math  
                     y(N-1)
          x(N-1) = ――――――――――
                   β(N-1, N-1)
  
                 N-1
        1  ┌─     ⎲        ─┐
  xi =  ―  │ yi - ⎳ βij · xj│ i = N-2, N-3....0
       βii │     j=i+1      │
           └─              ─┘


  @end

  *** Time complexity
      These two equations have N^2 executions of an inner loop containing one multiply and one add.
      If we have N right hand sides that are column vectors (like if we are inverting) then we have $1/6 N^3$ for y and $1/2 N^3$ for x. A distinct advantage of this is we can solve for as many right hand sides as we want







* Footnotes
  ^ Direct method: 
  calculates the solution in a predictable number of steps. Most common.
  ^ Indirect Method
  Calcualtes the solution iteratively, ideal when we want to avoid roundoff error as much as possible.
